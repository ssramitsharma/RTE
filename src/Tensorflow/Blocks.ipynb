{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Functions and components that can be slotted into tensorflow models.\n",
    "\n",
    "TODO: Write functions for various types of attention.\n",
    "\n",
    "\"\"\"\n",
    "# Reference : https://github.com/YichenGong/Densely-Interactive-Inference-Network\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    Get true length of sequences (without padding), and mask for true-length in max-length.\n",
    "\n",
    "    Input of shape: (batch_size, max_seq_length, hidden_dim)\n",
    "    Output shapes, \n",
    "    length: (batch_size)\n",
    "    mask: (batch_size, max_seq_length, 1)\n",
    "    \"\"\"\n",
    "    populated = tf.sign(tf.abs(sequence))\n",
    "    length = tf.cast(tf.reduce_sum(populated, axis=1), tf.int32)\n",
    "    mask = tf.cast(tf.expand_dims(populated, -1), tf.float32)\n",
    "    return length, mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def sign(x):\n",
    "    q = tf.sign(x)\n",
    "    sess = tf.Session()\n",
    "    print(sess.run(q))\n",
    "sign(4)\n",
    "sign(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it returns 0 if its positive and 1 if its negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abso(x):\n",
    "    x = -1\n",
    "    y = tf.abs(x)\n",
    "\n",
    "    print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question : why use absolute and then use sign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[2 2 2]\n",
      "[3 3]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "p = tf.reduce_sum(x)  # 6\n",
    "sess = tf.Session()\n",
    "print(sess.run(p))\n",
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "p = tf.reduce_sum(x,0)  \n",
    "sess = tf.Session()\n",
    "print(sess.run(p))\n",
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "p = tf.reduce_sum(x,1)  # 6\n",
    "sess = tf.Session()\n",
    "print(sess.run(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 1]\n"
     ]
    }
   ],
   "source": [
    "def cast(x):\n",
    "    q = tf.cast(x,tf.int32)\n",
    "    sess = tf.Session()\n",
    "    y = sess.run(q)\n",
    "    print(y)\n",
    "x = tf.constant([3.9,3.2,1.2])\n",
    "cast(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4]\n",
      " [4. ]\n",
      " [5. ]]\n"
     ]
    }
   ],
   "source": [
    "t = [2.4,4,5]\n",
    "p = tf.cast( tf.expand_dims(t, -1),tf.float32) # [1, 2]\n",
    "sess = tf.Session()\n",
    "print(sess.run(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biLSTM(inputs, dim, seq_len, name):\n",
    "    \"\"\"\n",
    "    A Bi-Directional LSTM layer. Returns forward and backward hidden states as a tuple, and cell states as a tuple.\n",
    "\n",
    "    Ouput of hidden states: [(batch_size, max_seq_length, hidden_dim), (batch_size, max_seq_length, hidden_dim)]\n",
    "    Same shape for cell states.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scope('forward' + name):\n",
    "            lstm_fwd = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "        with tf.variable_scope('backward' + name):\n",
    "            lstm_bwd = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "\n",
    "        hidden_states, cell_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fwd, cell_bw=lstm_bwd, inputs=inputs, sequence_length=seq_len, dtype=tf.float32, scope=name)\n",
    "\n",
    "    return hidden_states, cell_states\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def last_output(output, true_length):\n",
    "    \"\"\"\n",
    "    To get the last hidden layer form a dynamically unrolled RNN.\n",
    "    Input of shape (batch_size, max_seq_length, hidden_dim).\n",
    "\n",
    "    true_length: Tensor of shape (batch_size). Such a tensor is given by the length() function.\n",
    "    Output of shape (batch_size, hidden_dim).\n",
    "    \"\"\"\n",
    "    max_length = int(output.get_shape()[1])\n",
    "    length_mask = tf.expand_dims(tf.one_hot(true_length-1, max_length, on_value=1., off_value=0.), -1)\n",
    "    last_output = tf.reduce_sum(tf.multiply(output, length_mask), 1)\n",
    "    return last_output\n",
    "\n",
    "\n",
    "output = tf.constant([[1,2,3],[1,2,6]])\n",
    "max_length = int(output.get_shape()[1])\n",
    "print(max_length)\n",
    "\n",
    "indices = [0,1,2]\n",
    "depth = 4\n",
    "a = tf.one_hot(indices, depth) \n",
    "sess = tf.Session()\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(scores, mask):\n",
    "    \"\"\"\n",
    "    Used to calculcate a softmax score with true sequence length (without padding), rather than max-sequence length.\n",
    "\n",
    "    Input shape: (batch_size, max_seq_length, hidden_dim). \n",
    "    mask parameter: Tensor of shape (batch_size, max_seq_length). Such a mask is given by the length() function.\n",
    "    \"\"\"\n",
    "    numerator = tf.exp(tf.subtract(scores, tf.reduce_max(scores, 1, keep_dims=True))) * mask\n",
    "    denominator = tf.reduce_sum(numerator, 1, keep_dims=True)\n",
    "    weights = tf.div(numerator, denominator)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
