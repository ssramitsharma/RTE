{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Recognizing textual entailment\n",
    "* RTE is a corresponding decision problem whether a given(coherent) text T entails a given text H (in this context oftencalled a hypothesis)\n",
    "* Since RTE is a binary decision problem, in case of a negative result of RTE, i. e., when T does not entail H, it is not possible to state “how distant” is H from another hypothesis H ′ , such that H ′ is entailed by T . *From a different point of view, it is not possible to express that H is “almost entailed”by T in this setting.\n",
    "*  Many approaches and refinements of approaches have been considered, such as word embedding, logical models, graphical models, rule systems, contextual focusing, and machine learning\n",
    "* Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.\n",
    "* Software for training and using word embeddings includes Tomas Mikolov's Word2vec, Stanford University's GloVe, Gensim, Indra and Deeplearning. \n",
    "* Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.\n",
    "* Reference: Wikipaedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-03 13:14:02,099 : INFO : collecting all words and their counts\n",
      "2017-11-03 13:14:02,102 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-03 13:14:08,519 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-11-03 13:14:08,520 : INFO : Loading a fresh vocabulary\n",
      "2017-11-03 13:14:08,889 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-11-03 13:14:08,890 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-11-03 13:14:09,069 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-11-03 13:14:09,112 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-11-03 13:14:09,113 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-11-03 13:14:09,114 : INFO : estimated required memory for 71290 words and 200 dimensions: 149709000 bytes\n",
      "2017-11-03 13:14:09,367 : INFO : resetting layer weights\n",
      "2017-11-03 13:14:10,123 : INFO : training model with 3 workers on 71290 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-03 13:14:11,129 : INFO : PROGRESS: at 0.98% examples, 607171 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:12,130 : INFO : PROGRESS: at 1.98% examples, 611218 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:14:13,137 : INFO : PROGRESS: at 2.96% examples, 611120 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:14,148 : INFO : PROGRESS: at 3.96% examples, 612651 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:15,158 : INFO : PROGRESS: at 4.95% examples, 613317 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:16,171 : INFO : PROGRESS: at 5.94% examples, 614383 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:17,174 : INFO : PROGRESS: at 6.91% examples, 614393 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:14:18,182 : INFO : PROGRESS: at 7.90% examples, 614657 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:14:19,187 : INFO : PROGRESS: at 8.89% examples, 614712 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:20,187 : INFO : PROGRESS: at 9.86% examples, 614590 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:21,208 : INFO : PROGRESS: at 10.86% examples, 614715 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:22,215 : INFO : PROGRESS: at 11.86% examples, 615316 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:23,220 : INFO : PROGRESS: at 12.85% examples, 615441 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:24,225 : INFO : PROGRESS: at 13.82% examples, 614482 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:25,239 : INFO : PROGRESS: at 14.80% examples, 614304 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:14:26,246 : INFO : PROGRESS: at 15.83% examples, 614468 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:27,251 : INFO : PROGRESS: at 16.81% examples, 614464 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:28,254 : INFO : PROGRESS: at 17.79% examples, 614065 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:29,256 : INFO : PROGRESS: at 18.79% examples, 614352 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:30,265 : INFO : PROGRESS: at 19.76% examples, 613719 words/s, in_qsize 4, out_qsize 1\n",
      "2017-11-03 13:14:31,267 : INFO : PROGRESS: at 20.74% examples, 613519 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:32,275 : INFO : PROGRESS: at 21.73% examples, 613021 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:14:33,301 : INFO : PROGRESS: at 22.73% examples, 613026 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:34,299 : INFO : PROGRESS: at 23.69% examples, 612394 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:14:35,300 : INFO : PROGRESS: at 24.67% examples, 612398 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:36,306 : INFO : PROGRESS: at 25.64% examples, 612424 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-03 13:14:37,309 : INFO : PROGRESS: at 26.61% examples, 612302 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:38,330 : INFO : PROGRESS: at 27.60% examples, 612208 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:39,347 : INFO : PROGRESS: at 28.58% examples, 612114 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:40,351 : INFO : PROGRESS: at 29.58% examples, 612543 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:14:41,366 : INFO : PROGRESS: at 30.56% examples, 612227 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:42,366 : INFO : PROGRESS: at 31.52% examples, 612043 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:43,380 : INFO : PROGRESS: at 32.50% examples, 611778 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:44,382 : INFO : PROGRESS: at 33.47% examples, 611746 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:14:45,394 : INFO : PROGRESS: at 34.46% examples, 611762 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:46,414 : INFO : PROGRESS: at 35.46% examples, 611590 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:47,420 : INFO : PROGRESS: at 36.44% examples, 611184 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:48,439 : INFO : PROGRESS: at 37.41% examples, 610963 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-03 13:14:49,444 : INFO : PROGRESS: at 38.40% examples, 610902 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:50,449 : INFO : PROGRESS: at 39.29% examples, 609488 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:51,454 : INFO : PROGRESS: at 40.24% examples, 608853 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:52,464 : INFO : PROGRESS: at 41.20% examples, 608424 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-03 13:14:53,474 : INFO : PROGRESS: at 41.96% examples, 605153 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:54,486 : INFO : PROGRESS: at 42.86% examples, 603929 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:55,493 : INFO : PROGRESS: at 43.79% examples, 603299 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:56,512 : INFO : PROGRESS: at 44.75% examples, 603148 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:14:57,518 : INFO : PROGRESS: at 45.66% examples, 602384 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:14:58,528 : INFO : PROGRESS: at 46.63% examples, 602593 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:14:59,541 : INFO : PROGRESS: at 47.62% examples, 602945 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:00,535 : INFO : PROGRESS: at 48.57% examples, 602810 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:01,539 : INFO : PROGRESS: at 49.56% examples, 603090 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:02,539 : INFO : PROGRESS: at 50.53% examples, 603255 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:03,546 : INFO : PROGRESS: at 51.51% examples, 603377 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:04,567 : INFO : PROGRESS: at 52.51% examples, 603572 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:05,568 : INFO : PROGRESS: at 53.50% examples, 603833 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:06,580 : INFO : PROGRESS: at 54.47% examples, 603874 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:07,588 : INFO : PROGRESS: at 55.49% examples, 604029 words/s, in_qsize 4, out_qsize 1\n",
      "2017-11-03 13:15:08,597 : INFO : PROGRESS: at 56.47% examples, 604095 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:09,617 : INFO : PROGRESS: at 57.48% examples, 604368 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:10,626 : INFO : PROGRESS: at 58.48% examples, 604589 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:11,629 : INFO : PROGRESS: at 59.49% examples, 604960 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:12,638 : INFO : PROGRESS: at 60.48% examples, 605072 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:13,641 : INFO : PROGRESS: at 61.47% examples, 605089 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:14,651 : INFO : PROGRESS: at 62.47% examples, 605247 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:15,652 : INFO : PROGRESS: at 63.47% examples, 605532 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:15:16,659 : INFO : PROGRESS: at 64.44% examples, 605536 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:17,675 : INFO : PROGRESS: at 65.43% examples, 605674 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:18,685 : INFO : PROGRESS: at 66.42% examples, 605879 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:19,686 : INFO : PROGRESS: at 67.41% examples, 606142 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-03 13:15:20,707 : INFO : PROGRESS: at 68.41% examples, 606249 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:21,707 : INFO : PROGRESS: at 69.39% examples, 606450 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:22,708 : INFO : PROGRESS: at 70.37% examples, 606507 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:23,732 : INFO : PROGRESS: at 71.37% examples, 606615 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:24,750 : INFO : PROGRESS: at 72.36% examples, 606637 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:25,755 : INFO : PROGRESS: at 73.35% examples, 606758 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:26,768 : INFO : PROGRESS: at 74.34% examples, 606927 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:15:27,779 : INFO : PROGRESS: at 75.36% examples, 607022 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:28,793 : INFO : PROGRESS: at 76.33% examples, 606933 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:29,795 : INFO : PROGRESS: at 77.33% examples, 607107 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:30,798 : INFO : PROGRESS: at 78.32% examples, 607201 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:31,801 : INFO : PROGRESS: at 79.32% examples, 607361 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:32,804 : INFO : PROGRESS: at 80.31% examples, 607435 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-03 13:15:33,812 : INFO : PROGRESS: at 81.31% examples, 607527 words/s, in_qsize 6, out_qsize 1\n",
      "2017-11-03 13:15:34,817 : INFO : PROGRESS: at 82.30% examples, 607600 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:35,829 : INFO : PROGRESS: at 83.32% examples, 607793 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:36,833 : INFO : PROGRESS: at 84.29% examples, 607801 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:37,853 : INFO : PROGRESS: at 85.28% examples, 607832 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:38,868 : INFO : PROGRESS: at 86.28% examples, 608018 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:39,876 : INFO : PROGRESS: at 87.25% examples, 608113 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:40,874 : INFO : PROGRESS: at 88.23% examples, 608111 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:41,890 : INFO : PROGRESS: at 89.22% examples, 608141 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:15:42,907 : INFO : PROGRESS: at 90.22% examples, 608244 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:43,926 : INFO : PROGRESS: at 91.24% examples, 608520 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:44,926 : INFO : PROGRESS: at 92.24% examples, 608697 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:45,939 : INFO : PROGRESS: at 93.25% examples, 608918 words/s, in_qsize 5, out_qsize 1\n",
      "2017-11-03 13:15:46,949 : INFO : PROGRESS: at 94.25% examples, 609006 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:47,953 : INFO : PROGRESS: at 95.27% examples, 609189 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-03 13:15:48,959 : INFO : PROGRESS: at 96.27% examples, 609240 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:49,963 : INFO : PROGRESS: at 97.30% examples, 609511 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:50,975 : INFO : PROGRESS: at 98.31% examples, 609681 words/s, in_qsize 4, out_qsize 0\n",
      "2017-11-03 13:15:51,982 : INFO : PROGRESS: at 99.25% examples, 609377 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-03 13:15:52,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-03 13:15:52,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-03 13:15:52,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-03 13:15:52,730 : INFO : training on 85026035 raw words (62534680 effective words) took 102.6s, 609468 effective words/s\n",
      "2017-11-03 13:15:52,734 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.6264169216156006)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " # load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "sentences = word2vec.Text8Corpus('text8') # train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=200) # ... and some hours later... just as advertised...\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-03 13:15:52,831 : INFO : saving Word2Vec object under text8.model, separately None\n",
      "2017-11-03 13:15:52,838 : INFO : not storing attribute syn0norm\n",
      "2017-11-03 13:15:52,842 : INFO : storing np array 'syn0' to text8.model.wv.syn0.npy\n",
      "2017-11-03 13:15:53,067 : INFO : storing np array 'syn1neg' to text8.model.syn1neg.npy\n",
      "2017-11-03 13:15:53,329 : INFO : not storing attribute cum_table\n",
      "2017-11-03 13:15:54,040 : INFO : saved text8.model\n",
      "2017-11-03 13:15:54,041 : INFO : storing 71290x200 projection weights into text8.model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'mother', 0.7772825360298157), (u'wife', 0.7233453392982483), (u'grandmother', 0.7137593030929565)]\n",
      "'he' is to 'his' as 'she' is to 'her'\n",
      "'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "'going' is to 'went' as 'being' is to 'was'\n",
      "cereal\n",
      "chair\n",
      "orange\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "model.save('text8.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "model.wv.save_word2vec_format('text8.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "#model =gensim.models.KeyedVectors.load_word2vec_format('vectors.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "print model.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
    "#[('mother', 0.61849487), ('wife', 0.57972813), ('daughter', 0.56296098)]\n",
    "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "     a, b, x = example.split()\n",
    "     predicted = model.most_similar([x, b], [a])[0][0]\n",
    "     print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)\n",
    "\n",
    " \n",
    "# which word doesn't go with the others?\n",
    "print model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "print model.doesnt_match(\"table chair bed wall\".split())\n",
    "print model.doesnt_match(\"wine beer milk orange\".split())\n",
    "\n",
    "#Reference : https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Empty proposal abstract\n",
    "#Word2vec types \n",
    "#look deep into  word embeddings\n",
    "#different types of gensim on youtube\n",
    "#SNLP dataset info\n",
    "#converting vectors to sentence entailment\n",
    "#Brief overview of the top 3 methods\n",
    "#Lstm's\n",
    "#Zotero tool\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
